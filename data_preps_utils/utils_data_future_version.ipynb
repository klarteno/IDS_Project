{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Azure setup\n",
    "# install in notebook or in the console: conda activate env and install:\n",
    "# !conda install -c conda-forge --y imbalanced-learn\n",
    "# !conda install -c anaconda --y seaborn\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sys\n",
    "import os\n",
    "\n",
    "import glob\n",
    "from torchvision.transforms import ToTensor\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset,DataLoader\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, LabelEncoder #, OneHotEncoder\n",
    "\n",
    "from pprint import pprint\n",
    "import time\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive', force_remount=True)\n",
    "%cd '/content/drive/My Drive/1_MalmoUni/AdvML/Project'\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_column_name(column):\n",
    "    column = column.strip(' ')\n",
    "    column = column.replace('/', '_')\n",
    "    column = column.replace(' ', '_')\n",
    "    column = column.lower()\n",
    "    return column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATA_DIR  = os.path.join(os.path.abspath(\".\"), \"datasets\\MachineLearningCSV\\MachineLearningCVE\")\n",
    "DATA_DIR  = os.path.join(os.path.abspath(\".\"), \"datasets/MachineLearningCSV/MachineLearningCVE\")\n",
    "\n",
    "\n",
    "# Read all the .csv files\n",
    "filenames = glob.glob(os.path.join(DATA_DIR,  '*.csv'))\n",
    "datasets = [pd.read_csv(filename) for filename in filenames]\n",
    "\n",
    "# Remove white spaces and rename the columns\n",
    "for dataset in datasets:\n",
    "    dataset.columns = [clean_column_name(column) for column in dataset.columns]\n",
    "\n",
    "# Concatenate the datasets\n",
    "dataset = pd.concat(datasets, axis=0, ignore_index=True)\n",
    "#dataset.drop(labels=['fwd_header_length.1'], axis= 1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.label.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('The total number of rows duplicates are: ',dataset.duplicated(keep=False).sum())   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "duplicate = dataset[dataset.duplicated()]\n",
    "print('Duplicated rows by labels are: ')\n",
    "duplicate.label.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Datase duplicates :', dataset.duplicated().any())\n",
    "lenght_data = len(dataset)\n",
    "dataset.drop_duplicates(inplace=True, keep=False, ignore_index=True)\n",
    "# Remove duplicate rows\n",
    "dups_count = lenght_data-len(dataset)\n",
    "\n",
    "print('count of duplicate values dropped: ', dups_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.label.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_nans(label='NAN '):\n",
    "    result = dataset.isna().sum()\n",
    "    \n",
    "    for idx in result.index:\n",
    "        if result[idx] > 0:\n",
    "            print(idx,' has '+ label + ' values :' ,result[idx])      \n",
    "\n",
    "print_nans()  \n",
    "\n",
    "lenght_data = len(dataset)\n",
    "dataset.dropna(axis=0, inplace=True, how=\"any\")\n",
    "# Remove duplicate rows\n",
    "dups_count = lenght_data-len(dataset)\n",
    "\n",
    "print('count of NANs values dropped: ', dups_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('are all values finite: ',np.all(np.isfinite(dataset.drop(['label'], axis=1))))\n",
    "# Replace infinite values to NaN\n",
    "dataset.replace([-np.inf, np.inf], np.nan, inplace=True)\n",
    "print_nans(label='Inf ')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[(dataset['flow_bytes_s'].isna()) & (dataset['flow_packets_s'].isna())].label.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lenght_data = len(dataset)\n",
    "\n",
    "# Remove infinte values\n",
    "dataset.dropna(axis=0, how='any', inplace=True)\n",
    "\n",
    "dups_count = lenght_data-len(dataset)\n",
    "\n",
    "print('count of infinte values dropped: ', dups_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.label.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_datatypes = dataset.columns.to_series().groupby(dataset.dtypes).groups\n",
    "datatypes_info = {k.name: v for k, v in dataset_datatypes.items()}\n",
    "\n",
    "pprint(datatypes_info)\n",
    "pprint(dataset.describe(include=[object]).transpose())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[['flow_bytes_s', 'flow_packets_s']] = dataset[['flow_bytes_s', 'flow_packets_s']].apply(pd.to_numeric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = dataset.label\n",
    "X = dataset.drop(columns='label')\n",
    "\n",
    "class_labels = Y.unique()\n",
    "\n",
    "num_classes = Y.nunique()     # number of unique values\n",
    "print(\"shape of X: \",X.shape)\n",
    "print(\"number of labels of y: \", num_classes)\n",
    "print(\"Class labels: \", class_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  are there missing values\n",
    "'''\n",
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "mean_imp = SimpleImputer(missing_values=-1, strategy='mean')\n",
    "cat_imp = SimpleImputer(missing_values=-1, strategy='most_frequent')\n",
    "for c in X.columns:\n",
    "  X[c] = mean_imp.fit_transform(X[[c]]).ravel()\n",
    "  \n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('X shape: ', X.shape)\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, random_state=42, stratify=Y)\n",
    "\n",
    "print(\"\\nafter spliting the data:\")\n",
    "print(\"X training data shape:\", X_train.shape)\n",
    "print(\"x test data shape:\", X_test.shape)\n",
    "\n",
    "print(\"Y training data shape:\", Y_train.shape)\n",
    "print(\"Y test data shape:\", Y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "le = LabelEncoder()       # Encode target labels with value between 0 and n_classes-1\n",
    "\n",
    "Y_train_binary = le.fit_transform(Y_train)\n",
    "\n",
    "#print(\"instances per label in test set\\n\", y_test_binary.value_counts())\n",
    "# transform -\tTransform labels to normalized encoding.\n",
    "Y_test_binary = le.transform(Y_test)\n",
    "\n",
    "#we use fit_transform() on training data but transform() on the test data\n",
    "\n",
    "# classes_ - ndarray of shape (n_classes,) - Holds the label for each class.\n",
    "# To create a dictionary from two sequences, use dict(zip(keys, values))\n",
    "# The zip(fields, values) method returns an iterator that generates two-items tuples \n",
    "labels_dict = dict(zip(le.classes_, range(len(le.classes_))))\n",
    "\n",
    "pprint(labels_dict)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import pickle\n",
    "\n",
    "_file = open(\"datasets/labels_dict_file.pkl\",\"wb\")\n",
    "pickle.dump(labels_dict, _file)\n",
    "_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.DataFrame(Y_train)\n",
    "df.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.DataFrame(Y_train_binary)\n",
    "df.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.cpu_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "majority_feature_values= 1526628  \n",
    "dict_strategy={}\n",
    "\n",
    "for k in labels_dict.values():\n",
    "    if labels_dict['BENIGN']==k:\n",
    "        continue\n",
    "    dict_strategy[k]= int(15/100 * majority_feature_values)\n",
    "\n",
    "dict_strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "majority_feature_values= 1526628  \n",
    "\n",
    "dict_strategy=labels_dict.copy()\n",
    "\n",
    "for k in dict_strategy.keys():\n",
    "    dict_strategy[k]= int(15/100 * majority_feature_values)\n",
    "\n",
    "dict_strategy.pop('BENIGN')\n",
    "\n",
    "dict_strategy\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from imblearn.over_sampling import KMeansSMOTE\n",
    "\n",
    "k_smt = KMeansSMOTE(random_state=42, sampling_strategy=dict_strategy, k_neighbors=7,kmeans_estimator=20, n_jobs= os.cpu_count())\n",
    "\n",
    "x_sampled, y_sampled = k_smt.fit_resample(X_train, Y_train_binary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from imblearn.over_sampling import ADASYN\n",
    "\n",
    "adasyn_sampl = ADASYN(random_state=42, sampling_strategy=dict_strategy, n_neighbors=7, n_jobs= os.cpu_count())\n",
    "\n",
    "x_adasyn_sampled, y_adasyn_sampled = adasyn_sampl.fit_resample(X_train, Y_train_binary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.DataFrame(y_adasyn_sampled)\n",
    "df.value_counts() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt(\"datasets/y_train_78_features_ADASYN_sampled.csv\", y_adasyn_sampled, delimiter=\",\")\n",
    "np.savetxt(\"datasets/x_train_78_features_ADASYN_sampled.csv\", x_adasyn_sampled, delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "majority_feature_values= 1526628  \n",
    "\n",
    "dict_strategy_undersample=dict()\n",
    "dict_strategy_undersample[labels_dict['BENIGN']]=int(20/100 * majority_feature_values)\n",
    "dict_strategy_undersample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.under_sampling import InstanceHardnessThreshold\n",
    "\n",
    "iht = InstanceHardnessThreshold(random_state=42, sampling_strategy=dict_strategy_undersample, cv=15, n_jobs=os.cpu_count())\n",
    "\n",
    "x_undersampled_iht, y_undersampled_iht = iht.fit_resample(x_adasyn_sampled, y_adasyn_sampled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.DataFrame(y_undersampled_iht)\n",
    "df.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from numpy import genfromtxt\n",
    "\n",
    "labels_dict = pickle.load(open(\"datasets/labels_dict_file.pkl\", \"rb\"))\n",
    "\n",
    "x_undersampled_iht = genfromtxt(\"datasets/x_train_78_features_ADASYN_undersampled_iht.csv\", delimiter=',', autostrip=True)\n",
    "y_undersampled_iht = genfromtxt(\"datasets/y_train_78_features_ADASYN_undersampled_iht.csv\",dtype=np.uint8, delimiter=',', autostrip=True)\n",
    "\n",
    "print('x: ',x_undersampled_iht.shape)\n",
    "print('y: ',y_undersampled_iht.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_undersampled_iht"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "majority_feature_values= 1526628  \n",
    "\n",
    "dict_strategy_undersample=dict()\n",
    "dict_strategy_undersample[labels_dict['BENIGN']]=int(20/100 * majority_feature_values)\n",
    "dict_strategy_undersample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.under_sampling import InstanceHardnessThreshold\n",
    "import time\n",
    "\n",
    "\n",
    "iht = InstanceHardnessThreshold(random_state=42, sampling_strategy=dict_strategy_undersample, cv=10, n_jobs=os.cpu_count())\n",
    "for i in range(4):\n",
    "    x_undersampled_iht, y_undersampled_iht = iht.fit_resample(x_undersampled_iht, y_undersampled_iht)\n",
    "\n",
    "    print('runned instances:', i)\n",
    "    time.sleep(5)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.DataFrame(y_undersampled_iht) # 1261368\n",
    "df.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(x_undersampled_iht))\n",
    "type(y_undersampled_iht)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt(\"datasets/y_train_78_features_ADASYN_undersampled_iht.csv\", y_undersampled_iht, delimiter=\",\")\n",
    "np.savetxt(\"datasets/x_train_78_features_ADASYN_undersampled_iht.csv\", x_undersampled_iht, delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.DataFrame(x_undersampled_iht)\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.DataFrame(y_undersampled_iht)\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(Y_test_binary))\n",
    "type(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt(\"datasets/y_test_binary_78_features.csv\", Y_test_binary, delimiter=\",\")\n",
    "np.savetxt(\"datasets/x_test_78_features.csv\", X_test, delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.6 | packaged by conda-forge | (main, Aug 22 2022, 20:30:19) [MSC v.1929 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "201b6068cbed112f118c29a6393440d4ce4dc02105f31305c61d838eb972bb93"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
